# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nReByt5FqAuK0-uZYEut8I8coOQjsRJs
"""

!pip install pyprind

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import gridspec
import os
import pyprind 
import seaborn as sns
from pylab import rcParams
from collections import defaultdict
from scipy.optimize import curve_fit
from collections import Counter
import sys
import warnings
if not sys.warnoptions:
    warnings.simplefilter("ignore")

!pip install --force https://github.com/chengs/tqdm/archive/colab.zip
from tqdm import tqdm_notebook as tqdm

from google.colab import drive
drive.mount('/content/drive/')

!ls "/content/drive/My Drive/dev-set"

from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(1)

def read_caps(fname):
    """Load the captions into a dataframe"""
    vn = []
    cap = []
    df = pd.DataFrame();
    with open(fname) as f:
        for line in f:
            pairs = line.split()
            vn.append(pairs[0])
            cap.append(pairs[1])
        df['video']=vn
        df['caption']=cap
    return df


# load the captions
caption_file = '/content/drive/My Drive/dev-set/dev-set_video-captions.txt'
df_cap = read_caps(caption_file)

# load the ground truth values
ground_truth_file = '/content/drive/My Drive/dev-set/dev-set_ground-truth.csv'
ground_truth = pd.read_csv(ground_truth_file)

print(ground_truth)

from string import punctuation
counts = Counter()
# setup prograss tracker
pbar = pyprind.ProgBar(len(df_cap['caption']), title='Counting word occurrences')
for i, cap in enumerate(df_cap['caption']):
    # replace punctuations with space
    # convert words to lower case 
    text = ''.join([c if c not in punctuation else ' ' for c in cap]).lower()
    df_cap.loc[i,'caption'] = text
    pbar.update()
    counts.update(text.split())

from keras.preprocessing.text import Tokenizer
# build the word index
len_token = len(counts)
tokenizer = Tokenizer(num_words=len_token)
print(len_token)

tokenizer.fit_on_texts(list(df_cap.caption.values)) #fit a list of captions to the tokenizer
#the tokenizer vectorizes a text corpus, by turning each text into either a sequence of integers

print(len(tokenizer.word_index))

one_hot_res = tokenizer.texts_to_matrix(list(df_cap.caption.values),mode='binary')
sequences = tokenizer.texts_to_sequences(list(df_cap.caption.values))

#Just to visualise some stuff in sequences and counts
print(sequences[0]) # prints location of words from caption 0 'blonde woman is massaged tilt down'
print(counts['blonde']) # no. of occurences of 'blonde'
n=3
print('Least Common: ', counts.most_common()[:-n-1:-1])       # n least common elements
print('Most Common: ',counts.most_common(n))

# calculating max length
max_len = 50

print(sequences[0]) # length of 1st sequence

X_seq = np.zeros((len(sequences),max_len))
for i in range(len(sequences)):
    n = len(sequences[i])
    if n==0:
        print(i)
    else:
        X_seq[i,-n:] = sequences[i]
X_seq.shape

X_seq[0,:]

Y = ground_truth[['short-term_memorability','long-term_memorability']].values
X = X_seq # sequences

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)

X_train.shape

!pip list | grep -i keras

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import GRU
from keras.layers import Embedding
from keras.layers import Masking
from keras.layers import LSTM
from keras.layers import Conv2D
from keras.layers import Dropout

np.random.seed(0)

model = Sequential()
# Embedding layer
model.add(Embedding(input_dim=5191, output_dim = 20, input_length=50, init='uniform'))

model.add(LSTM(200, activation='relu', recurrent_initializer='orthogonal'))
# Fully connected layer
model.add(Dense(10, activation='relu'))
# Dropout for regularization
model.add(Dropout(0.5))
# Output layer
model.add(Dense(2, activation='sigmoid'))
# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])

model.fit(X_train, Y_train, epochs = 5, validation_data=(X_test, Y_test))

train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

training_error = model.evaluate(X_train, Y_train, verbose = 1)
print('training error = ' + str(training_error))
testing_error = model.evaluate(X_test, Y_test, verbose = 1)
print('training error = ' + str(testing_error))

def Get_score(Y_pred,Y_true):
    '''Calculate the Spearmann"s correlation coefficient'''
    Y_pred = np.squeeze(Y_pred)
    Y_true = np.squeeze(Y_true)
    if Y_pred.shape != Y_true.shape:
        print('Input shapes don\'t match!')
    else:
        if len(Y_pred.shape) == 1:
            Res = pd.DataFrame({'Y_true':Y_true,'Y_pred':Y_pred})
            score_mat = Res[['Y_true','Y_pred']].corr(method='spearman',min_periods=1)
            print('The Spearman\'s correlation coefficient is: %.3f' % score_mat.iloc[1][0])
        else:
            for ii in range(Y_pred.shape[1]):
                Get_score(Y_pred[:,ii],Y_true[:,ii])

Get_score(test_predict, Y_test)