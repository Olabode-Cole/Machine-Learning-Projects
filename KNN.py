# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ARJcC_dLDQ6pWMuEf8YE-HHw8MKJmFdL
"""

!pip install pyprind

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import gridspec
import os
import pyprind 
import seaborn as sns
from pylab import rcParams
from collections import defaultdict
from scipy.optimize import curve_fit
from sklearn.model_selection import train_test_split
from collections import Counter
import sys
import warnings
if not sys.warnoptions:
    warnings.simplefilter("ignore")

!pip install --force https://github.com/chengs/tqdm/archive/colab.zip
from tqdm import tqdm_notebook as tqdm

from google.colab import drive
drive.mount('/content/drive/')

from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(1)

# load labels and captions
def read_caps(fname):
    """Load the captions into a dataframe"""
    vn = []
    cap = []
    df = pd.DataFrame();
    with open(fname) as f:
        for line in f:
            pairs = line.split()
            vn.append(pairs[0])
            cap.append(pairs[1])
        df['video']=vn
        df['caption']=cap
    return df


# load the captions
caption_file = '/content/drive/My Drive/dev-set/dev-set_video-captions.txt'
df_cap = read_caps(caption_file)

# load the ground truth values
ground_truth_file = '/content/drive/My Drive/dev-set/dev-set_ground-truth.csv'
ground_truth = pd.read_csv(ground_truth_file)

print(ground_truth)

from string import punctuation
counts = Counter()
# setup prograss tracker
pbar = pyprind.ProgBar(len(df_cap['caption']), title='Counting word occurrences')
for i, cap in enumerate(df_cap['caption']):
    # replace punctuations with space
    # convert words to lower case 
    text = ''.join([c if c not in punctuation else ' ' for c in cap]).lower()
    df_cap.loc[i,'caption'] = text
    pbar.update()
    counts.update(text.split())

from keras.preprocessing.text import Tokenizer
# build the word index
len_token = len(counts)
tokenizer = Tokenizer(num_words=len_token)
print(len_token)

tokenizer.fit_on_texts(list(df_cap.caption.values)) #fit a list of captions to the tokenizer
#the tokenizer vectorizes a text corpus, by turning each text into either a sequence of integers

print(len(tokenizer.word_index))

one_hot_res = tokenizer.texts_to_matrix(list(df_cap.caption.values),mode='binary')
sequences = tokenizer.texts_to_sequences(list(df_cap.caption.values))

#Just to visualise some stuff in sequences and counts
print(sequences[0]) # prints location of words from caption 0 'blonde woman is massaged tilt down'
print(counts['blonde']) # no. of occurences of 'blonde'
n=3
print('Least Common: ', counts.most_common()[:-n-1:-1])       # n least common elements
print('Most Common: ',counts.most_common(n))

# calculating max length
max_len = 50

print(sequences[0]) # length of 1st sequence

X_seq = np.zeros((len(sequences),max_len))
for i in range(len(sequences)):
    n = len(sequences[i])
    if n==0:
        print(i)
    else:
        X_seq[i,-n:] = sequences[i]
X_seq.shape

X_seq[0,:]

Y = ground_truth[['short-term_memorability','long-term_memorability']].values
X = X_seq # sequences

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1)
X_train.shape

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X=scaler.fit_transform(X)
X

from sklearn.decomposition import PCA
pca = PCA(n_components=0)
pca.fit(X)

!pip list | grep -i keras



from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.decomposition import PCA
from sklearn import linear_model
from sklearn.svm import SVR
from sklearn.model_selection import GridSearchCV
import seaborn as sns
import matplotlib.pyplot as plt

param_grid = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},
 ]

Y_train.reshape

pca = PCA()  
X_train = pca.fit_transform(X_train)  
X_test = pca.transform(X_test)

classifier = KNeighborsClassifier(n_neighbors=11,p=2,metric='euclidean')

classifier.fit(X_train,Y_train)
y_pred = classifier.predict(X_test)
x_pred = classifier.predict(Y_test)

classifier.append(('SVM', SVR()))


cm = confusion_matrix(y_test, Y_pred)  
print(cm)  
print('Accuracy', accuracy_score(Y_test, Y_pred))


# classifier = RandomForestClassifier(max_depth=2, random_state=0)  
# classifier.fit(X_train,Y_test)

# Y_pred = classifier.predict(X_test)

conf_matrix = confusion_matrix(Y_test,Y_pred)
print(conf_matrix)
print(f1_score(Y_test,Y_pred))

print(accuracy_score(Y_test,Y_pred))